{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12458011,"sourceType":"datasetVersion","datasetId":7858693}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms, models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom PIL import Image\nimport random\nimport platform\nimport psutil\nfrom urllib.error import URLError\nfrom tabulate import tabulate\nimport time\n\n# === Set random seed for reproducibility ===\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n# === Device ===\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# === Hardware information ===\ncpu_info = platform.processor()\nram_info = psutil.virtual_memory()\ntotal_ram_gb = ram_info.total / (1024 ** 3)\ntry:\n    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\"\nexcept:\n    gpu_name = \"No GPU available\"\nprint(f\"CPU: {cpu_info}\")\nprint(f\"Total RAM (GB): {round(total_ram_gb, 2)}\")\nprint(f\"GPU: {gpu_name}\")\n\n# === Data preparation ===\ndata_dir = \"/kaggle/input/classification-of-rice-varieties-in-bangladesh/An Extensive Image Dataset for Classifying Rice Varieties in Bangladesh/Augmented/Augmented\"\nfile_paths = []\nlabels = []\n\nif not os.path.exists(data_dir):\n    raise FileNotFoundError(f\"Dataset directory {data_dir} does not exist.\")\n\nfor class_name in os.listdir(data_dir):\n    class_dir = os.path.join(data_dir, class_name)\n    if os.path.isdir(class_dir):\n        for image_name in os.listdir(class_dir):\n            file_paths.append(os.path.join(class_dir, image_name))\n            labels.append(class_name)\n\ndf = pd.DataFrame({\"file_path\": file_paths, \"label\": labels})\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# === Display class distribution ===\nclass_counts = df['label'].value_counts()\nplt.figure(figsize=(10, 6))\nax = class_counts.plot(kind='bar')\nplt.xlabel('Classes')\nplt.ylabel('Number of Images')\nplt.title('Class Distribution')\nplt.xticks(rotation=45, ha='right')\nfor i, count in enumerate(class_counts):\n    ax.text(i, count + 5, str(count), ha='center')\nplt.ylim(0, max(class_counts) * 1.2)\nplt.tight_layout()\nplt.savefig('/kaggle/working/class_distribution.png')  # Save plot\nplt.close()\n\n# === Display a random sample image ===\nrandom_index = random.randint(0, len(df) - 1)\nrandom_row = df.iloc[random_index]\nimage = Image.open(random_row['file_path'])\nplt.figure(figsize=(6, 6))\nplt.imshow(image)\nplt.title(f\"Label: {random_row['label']}\")\nplt.axis('off')\nplt.savefig('/kaggle/working/sample_image.png')  # Save plot\nplt.close()\n\n# === Train/Validation/Test split ===\ntrain_df, temp_df = train_test_split(df, test_size=0.30, stratify=df['label'], random_state=42)\nvalid_df, test_df = train_test_split(temp_df, test_size=0.50, stratify=temp_df['label'], random_state=42)\n\nprint(f\"Training Data: {len(train_df)}\")\nprint(f\"Validation Data: {len(valid_df)}\")\nprint(f\"Test Data: {len(test_df)}\")\nprint(f\"Total Data: {len(df)}\")\n\n# === Display split distributions ===\ndef print_split_distribution(df, title):\n    counts = df['label'].value_counts()\n    table_data = [[class_name, count] for class_name, count in counts.items()]\n    print(f\"\\n{title}\")\n    print(tabulate(table_data, headers=[\"Class\", \"Count\"]))\n\nprint_split_distribution(train_df, \"Train Dataset\")\nprint_split_distribution(valid_df, \"Validation Dataset\")\nprint_split_distribution(test_df, \"Test Dataset\")\n\n# === Custom Dataset ===\nclass CustomImageDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n        self.label_map = {label: idx for idx, label in enumerate(sorted(set(dataframe['label'])))}\n        self.class_names = sorted(set(dataframe['label']))\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['file_path']\n        label = self.label_map[self.dataframe.iloc[idx]['label']]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# === Data transforms ===\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nvalid_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# === Datasets and DataLoaders ===\ntry:\n    train_dataset = CustomImageDataset(train_df, transform=train_transform)\n    valid_dataset = CustomImageDataset(valid_df, transform=valid_transform)\n    test_dataset = CustomImageDataset(test_df, transform=valid_transform)\nexcept Exception as e:\n    print(f\"Error creating datasets: {e}\")\n    raise\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# === Modified Model Initialization with Error Handling ===\ndef initialize_model(model_name, num_classes):\n    try:\n        print(f\"Attempting to download pretrained weights for {model_name}...\")\n        if model_name == \"convnext_tiny\":\n            model = models.convnext_tiny(weights='IMAGENET1K_V1')\n            num_ftrs = model.classifier[2].in_features\n            model.classifier[2] = nn.Linear(num_ftrs, num_classes)\n        elif model_name == \"mobilenet_v3_small\":\n            model = models.mobilenet_v3_small(weights='IMAGENET1K_V1')\n            num_ftrs = model.classifier[-1].in_features\n            model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n        else:\n            raise ValueError(f\"Unsupported model: {model_name}\")\n        print(\"Successfully loaded pretrained weights!\")\n    except URLError as e:\n        print(f\"Failed to download pretrained weights: {e}\")\n        print(\"Falling back to random initialization...\")\n        if model_name == \"convnext_tiny\":\n            model = models.convnext_tiny(weights=None)\n            num_ftrs = model.classifier[2].in_features\n            model.classifier[2] = nn.Linear(num_ftrs, num_classes)\n        elif model_name == \"mobilenet_v3_small\":\n            model = models.mobilenet_v3_small(weights=None)\n            num_ftrs = model.classifier[-1].in_features\n            model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n    return model.to(device)\n\n# === Training function with checkpointing ===\ndef train_model(model, model_name, criterion, optimizer, scheduler, \n                train_loader, valid_loader, num_epochs=20, \n                early_stop_patience=5, save_path_checkpoints=\"checkpoints\"):\n    \n    os.makedirs(save_path_checkpoints, exist_ok=True)\n    history_csv_path = os.path.join(save_path_checkpoints, f\"{model_name}_training_history.csv\")\n    last_checkpoint_path = os.path.join(save_path_checkpoints, f\"{model_name}_last.pt\")\n    \n    # Initialize training state\n    start_epoch = 0\n    train_loss_history = []\n    train_acc_history = []\n    val_loss_history = []\n    val_acc_history = []\n    best_val_acc = 0.0\n    consecutive_no_improvement = 0\n    num_epochs_loss_greater = 0\n    \n    # Check for existing checkpoint to resume\n    if os.path.exists(last_checkpoint_path):\n        print(f\"Resuming training from checkpoint: {last_checkpoint_path}\")\n        try:\n            checkpoint = torch.load(last_checkpoint_path, map_location=device)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            if 'scheduler_state_dict' in checkpoint:\n                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            \n            start_epoch = checkpoint['epoch'] + 1\n            train_loss_history = checkpoint['train_loss_history']\n            train_acc_history = checkpoint['train_acc_history']\n            val_loss_history = checkpoint['val_loss_history']\n            val_acc_history = checkpoint['val_acc_history']\n            best_val_acc = checkpoint['best_val_acc']\n            consecutive_no_improvement = checkpoint['consecutive_no_improvement']\n            num_epochs_loss_greater = checkpoint['num_epochs_loss_greater']\n            \n            print(f\"Resuming from epoch {start_epoch} | Previous best val acc: {best_val_acc:.4f}\")\n        except Exception as e:\n            print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n    \n    # Create or append to history CSV\n    if not os.path.exists(history_csv_path) or start_epoch == 0:\n        with open(history_csv_path, 'w') as f:\n            f.write(\"Epoch,Train Loss,Train Accuracy,Validation Loss,Validation Accuracy\\n\")\n    else:\n        print(f\"Appending to existing history: {history_csv_path}\")\n\n    # Main training loop\n    for epoch in range(start_epoch, num_epochs):\n        epoch_start_time = time.time()\n        \n        # Training\n        model.train()\n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        for inputs, labels in progress_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, predicted = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n            progress_bar.set_postfix(loss=running_loss/total_train, acc=correct_train/total_train)\n\n        epoch_train_loss = running_loss / total_train\n        epoch_train_acc = correct_train / total_train\n        train_loss_history.append(epoch_train_loss)\n        train_acc_history.append(epoch_train_acc)\n        print(f'Training Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n\n        # Validation\n        model.eval()\n        running_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        with torch.no_grad():\n            for inputs, labels in valid_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                _, predicted = torch.max(outputs, 1)\n                running_loss += loss.item() * inputs.size(0)\n                total_val += labels.size(0)\n                correct_val += (predicted == labels).sum().item()\n\n        epoch_val_loss = running_loss / total_val\n        epoch_val_acc = correct_val / total_val\n        val_loss_history.append(epoch_val_loss)\n        val_acc_history.append(epoch_val_acc)\n        print(f'Validation Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')\n\n        # Step scheduler\n        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(epoch_val_acc)\n        else:\n            scheduler.step()\n\n        # Save checkpoint after every epoch\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_loss_history': train_loss_history,\n            'train_acc_history': train_acc_history,\n            'val_loss_history': val_loss_history,\n            'val_acc_history': val_acc_history,\n            'best_val_acc': best_val_acc,\n            'consecutive_no_improvement': consecutive_no_improvement,\n            'num_epochs_loss_greater': num_epochs_loss_greater\n        }\n        \n        try:\n            torch.save(checkpoint, last_checkpoint_path)\n            # Append to CSV history\n            with open(history_csv_path, 'a') as f:\n                f.write(f\"{epoch+1},{epoch_train_loss:.4f},{epoch_train_acc:.4f},{epoch_val_loss:.4f},{epoch_val_acc:.4f}\\n\")\n            print(f\"Checkpoint saved for epoch {epoch+1}\")\n        except Exception as e:\n            print(f\"Error saving checkpoint: {e}\")\n\n        # Save best model separately\n        if epoch_val_acc > best_val_acc:\n            best_val_acc = epoch_val_acc\n            best_checkpoint_path = os.path.join(save_path_checkpoints, f\"{model_name}_best.pt\")\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'val_acc': best_val_acc\n            }, best_checkpoint_path)\n            print(f\"New best model saved with validation accuracy: {best_val_acc:.4f}\")\n            consecutive_no_improvement = 0\n        else:\n            consecutive_no_improvement += 1\n\n        # Check for validation loss being greater than training loss\n        if epoch_val_loss > epoch_train_loss:\n            num_epochs_loss_greater += 1\n        else:\n            num_epochs_loss_greater = 0\n\n        # Early stopping check\n        stop_reason = None\n        if consecutive_no_improvement >= early_stop_patience:\n            stop_reason = f\"Validation accuracy not improving for {early_stop_patience} epochs\"\n        if num_epochs_loss_greater >= early_stop_patience:\n            stop_reason = f\"Validation loss > training loss for {early_stop_patience} epochs\"\n        \n        if stop_reason:\n            print(f\"Early stopping triggered after epoch {epoch+1}: {stop_reason}\")\n            break\n\n        epoch_time = time.time() - epoch_start_time\n        print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds\")\n\n    return train_loss_history, train_acc_history, val_loss_history, val_acc_history\n\n# === Evaluation function ===\ndef evaluate_model(model, dataloader, class_names, model_name):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    running_loss = 0.0\n    total = 0\n    criterion = nn.CrossEntropyLoss()\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=f\"Evaluating {model_name}\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n            running_loss += loss.item() * inputs.size(0)\n            total += labels.size(0)\n\n    avg_loss = running_loss / total\n    avg_acc = (np.array(all_labels) == np.array(all_preds)).mean()\n    print(f\"\\n{model_name} Test Loss: {avg_loss:.4f}\")\n    print(f\"{model_name} Test Accuracy: {avg_acc:.4f}\")\n\n    print(f\"\\n{model_name} Classification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n\n    cm = confusion_matrix(all_labels, all_preds)\n    print(f\"\\n{model_name} Confusion Matrix:\")\n    print(cm)\n\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'{model_name} Confusion Matrix')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/{model_name}_confusion_matrix.png')\n    plt.close()\n\n    return all_labels, all_preds, cm, avg_loss, avg_acc\n\n# === Main Execution ===\nmodels_to_train = [\"convnext_tiny\", \"mobilenet_v3_small\"]\nnum_classes = len(set(df['label']))\nsave_path_checkpoints = \"/kaggle/working/checkpoints\"\nos.makedirs(save_path_checkpoints, exist_ok=True)\n\n# Set cache directory for pretrained models\nos.environ['TORCH_HOME'] = '/kaggle/working/pretrained_models'\n\nfor model_name in models_to_train:\n    print(f\"\\n{'='*40}\")\n    print(f\"=== Training {model_name} ===\")\n    print(f\"{'='*40}\\n\")\n    \n    model = initialize_model(model_name, num_classes)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.1, patience=3, verbose=True\n    )\n\n    # Train with checkpointing\n    train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(\n        model, model_name, criterion, optimizer, scheduler, \n        train_loader, valid_loader, num_epochs=50,\n        save_path_checkpoints=save_path_checkpoints\n    )\n\n    # Plot training history\n    if train_loss_history:\n        epochs = range(1, len(train_loss_history) + 1)\n        plt.figure(figsize=(12, 4))\n        \n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, train_loss_history, 'b-', label='Train Loss')\n        plt.plot(epochs, val_loss_history, 'r-', label='Validation Loss')\n        plt.title(f'{model_name} Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, train_acc_history, 'b-', label='Train Accuracy')\n        plt.plot(epochs, val_acc_history, 'r-', label='Validation Accuracy')\n        plt.title(f'{model_name} Accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig(f'/kaggle/working/{model_name}_training_history.png')\n        plt.close()\n\n    # Load best model for evaluation\n    best_model_path = os.path.join(save_path_checkpoints, f\"{model_name}_best.pt\")\n    if os.path.exists(best_model_path):\n        try:\n            checkpoint = torch.load(best_model_path, map_location=device)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            best_epoch = checkpoint['epoch'] + 1\n            print(f\"Loaded best model from epoch {best_epoch} with validation accuracy: {checkpoint['val_acc']:.4f}\")\n        except Exception as e:\n            print(f\"Error loading best model: {e}. Using final model state.\")\n    else:\n        print(f\"No best model found for {model_name}. Using final model state.\")\n\n    # Evaluate on test set\n    class_names = train_dataset.class_names\n    if len(class_names) != num_classes:\n        print(f\"Warning: Dataset has {len(class_names)} classes, but model expects {num_classes} classes.\")\n    \n    print(f\"\\n{'='*40}\")\n    print(f\"=== Evaluating {model_name} on Test Set ===\")\n    print(f\"{'='*40}\\n\")\n    \n    labels, preds, cm, test_loss, test_acc = evaluate_model(\n        model, test_loader, class_names, model_name\n    )\n    \n    # Save evaluation results\n    with open(f'/kaggle/working/{model_name}_test_results.txt', 'w') as f:\n        f.write(f\"Test Loss: {test_loss:.4f}\\n\")\n        f.write(f\"Test Accuracy: {test_acc:.4f}\\n\\n\")\n        f.write(\"Classification Report:\\n\")\n        f.write(classification_report(labels, preds, target_names=class_names))\n        f.write(\"\\nConfusion Matrix:\\n\")\n        f.write(str(cm))\n\nprint(\"\\nTraining and evaluation completed successfully!\")\nprint(\"All checkpoints and results saved to /kaggle/working\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T04:50:09.406774Z","iopub.execute_input":"2025-07-22T04:50:09.407435Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nCPU: x86_64\nTotal RAM (GB): 31.35\nGPU: Tesla P100-PCIE-16GB\nTraining Data: 53200\nValidation Data: 11400\nTest Data: 11400\nTotal Data: 76000\n\nTrain Dataset\nClass         Count\n----------  -------\nBR22           1400\nBRRI67         1400\nBinadhan14     1400\nBinadhan25     1400\nBD57           1400\nBRRI102        1400\nBD30           1400\nBD33           1400\nBD70           1400\nBinadhan7      1400\nBR23           1400\nBinadhan17     1400\nBD95           1400\nBD72           1400\nBinadhan20     1400\nBinadhan8      1400\nBD91           1400\nBD93           1400\nBD79           1400\nBD51           1400\nBD75           1400\nBinadhan11     1400\nBD52           1400\nBD49           1400\nBinadhan26     1400\nBinadhan19     1400\nBD76           1400\nBD87           1400\nBD56           1400\nBinadhan21     1400\nBinadhan12     1400\nBD39           1400\nBinadhan16     1400\nBD85           1400\nBinadhan24     1400\nBinadhan23     1400\nBRRI74         1400\nBinadhan10     1400\n\nValidation Dataset\nClass         Count\n----------  -------\nBD30            300\nBinadhan16      300\nBD52            300\nBRRI74          300\nBD57            300\nBinadhan8       300\nBD76            300\nBD33            300\nBD56            300\nBD72            300\nBR23            300\nBinadhan24      300\nBinadhan11      300\nBinadhan25      300\nBinadhan19      300\nBD39            300\nBinadhan20      300\nBinadhan12      300\nBinadhan10      300\nBD79            300\nBinadhan17      300\nBinadhan26      300\nBinadhan7       300\nBinadhan14      300\nBD51            300\nBD70            300\nBD93            300\nBinadhan21      300\nBD75            300\nBD85            300\nBR22            300\nBD95            300\nBRRI67          300\nBinadhan23      300\nBRRI102         300\nBD49            300\nBD91            300\nBD87            300\n\nTest Dataset\nClass         Count\n----------  -------\nBinadhan11      300\nBD52            300\nBinadhan10      300\nBD79            300\nBinadhan25      300\nBD85            300\nBD93            300\nBD75            300\nBD51            300\nBRRI102         300\nBinadhan17      300\nBD39            300\nBD56            300\nBD33            300\nBD49            300\nBinadhan8       300\nBD57            300\nBinadhan24      300\nBRRI67          300\nBRRI74          300\nBinadhan16      300\nBinadhan7       300\nBD91            300\nBinadhan14      300\nBinadhan19      300\nBD87            300\nBD72            300\nBinadhan26      300\nBD70            300\nBinadhan12      300\nBR22            300\nBinadhan21      300\nBinadhan20      300\nBD76            300\nBD95            300\nBR23            300\nBD30            300\nBinadhan23      300\n\n========================================\n=== Training convnext_tiny ===\n========================================\n\nAttempting to download pretrained weights for convnext_tiny...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Successfully loaded pretrained weights!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 1663/1663 [20:05<00:00,  1.38it/s, acc=0.801, loss=0.648]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6476 Acc: 0.8009\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.2057 Acc: 0.9309\nCheckpoint saved for epoch 1\nNew best model saved with validation accuracy: 0.9309\nEpoch 1 completed in 1273.40 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.938, loss=0.188]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1876 Acc: 0.9376\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1702 Acc: 0.9375\nCheckpoint saved for epoch 2\nNew best model saved with validation accuracy: 0.9375\nEpoch 2 completed in 1255.58 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.964, loss=0.111]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.1109 Acc: 0.9642\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0855 Acc: 0.9685\nCheckpoint saved for epoch 3\nNew best model saved with validation accuracy: 0.9685\nEpoch 3 completed in 1270.00 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.972, loss=0.0853]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0853 Acc: 0.9717\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0702 Acc: 0.9761\nCheckpoint saved for epoch 4\nNew best model saved with validation accuracy: 0.9761\nEpoch 4 completed in 1251.81 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.978, loss=0.0673]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0673 Acc: 0.9779\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0584 Acc: 0.9800\nCheckpoint saved for epoch 5\nNew best model saved with validation accuracy: 0.9800\nEpoch 5 completed in 1251.97 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.982, loss=0.0539]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0539 Acc: 0.9825\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0694 Acc: 0.9774\nCheckpoint saved for epoch 6\nEpoch 6 completed in 1254.33 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.983, loss=0.0547]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0547 Acc: 0.9826\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0356 Acc: 0.9879\nCheckpoint saved for epoch 7\nNew best model saved with validation accuracy: 0.9879\nEpoch 7 completed in 1255.68 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.985, loss=0.044] ","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0440 Acc: 0.9855\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0809 Acc: 0.9732\nCheckpoint saved for epoch 8\nEpoch 8 completed in 1282.41 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.986, loss=0.0439]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0439 Acc: 0.9855\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0319 Acc: 0.9891\nCheckpoint saved for epoch 9\nNew best model saved with validation accuracy: 0.9891\nEpoch 9 completed in 1252.21 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.987, loss=0.0382]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0382 Acc: 0.9871\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0511 Acc: 0.9824\nCheckpoint saved for epoch 10\nEpoch 10 completed in 1251.84 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 1663/1663 [20:06<00:00,  1.38it/s, acc=0.989, loss=0.0341]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.0341 Acc: 0.9892\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0394 Acc: 0.9854\nCheckpoint saved for epoch 11\nEpoch 11 completed in 1253.14 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50:  44%|████▍     | 735/1663 [08:53<11:13,  1.38it/s, acc=0.989, loss=0.0335]","output_type":"stream"}],"execution_count":null}]}